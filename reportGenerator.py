from datetime import datetime

def generate_text_report(model_name, fairness_metrics, fairness_interpretation, privacy_results, ethical_score,
                         shap_explanation=None, lime_explanation=None,
                         shap_explanation_text=None, lime_explanation_text=None):
    lines = []

    lines.append("MODEL EVALUATION REPORT")
    lines.append("Generated by EthicAI") 
    lines.append("=" * 50)

    lines.append(f"Model Name: {model_name}")
    lines.append(f"Report Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}")
    lines.append("")

    # ETHICAL SCORE
    lines.append("ETHICAL SCORE SUMMARY")
    lines.append("-" * 30)
    lines.append(f"Overall Ethical Score: {ethical_score['Ethical Score']} ({ethical_score['Interpretation']})")
    lines.append("Breakdown:")
    for k, v in ethical_score["Breakdown"].items():
        lines.append(f" - {k}: {v}")
    lines.append("")

    # FAIRNESS
    lines.append("FAIRNESS METRICS")
    lines.append("-" * 30)
    for metric, value in fairness_metrics.items():
        if isinstance(value, dict):
            lines.append(f"{metric}:")
            for group, val in value.items():
                val_rounded = round(val, 4) if isinstance(val, (float, int)) else val
                lines.append(f"  {group}: {val_rounded}")
        else:
            val_rounded = round(value, 4) if isinstance(value, (float, int)) else value
            lines.append(f"{metric}: {val_rounded}")
        lines.append(f"→ {fairness_interpretation.get(metric, 'No interpretation available')}")
        lines.append("")

    # PRIVACY
    lines.append("PRIVACY EVALUATION")
    lines.append("-" * 30)
    for check, result in privacy_results.items():
        lines.append(f"{check}: {result['Risk Level']}")
        lines.append(f"→ {result['Interpretation']}")
        for key, val in result.get("Details", {}).items():
            lines.append(f"    • {key}: {val}")
        lines.append("")

    # SHAP Explanation summary text
    if shap_explanation_text:
        lines.append("SHAP EXPLANATION")
        lines.append("-" * 30)
        lines.append(shap_explanation_text.strip())
        lines.append("")

    # SHAP feature importances
    if shap_explanation and "top_features" in shap_explanation:
        lines.append("SHAP Feature Importances")
        lines.append("-" * 30)
        for f in shap_explanation["top_features"]:
            lines.append(f"{f['feature']}: {f['importance']}")
        lines.append("")

    # LIME Explanation summary text
    if lime_explanation_text:
        lines.append("LIME EXPLANATION")
        lines.append("-" * 30)
        lines.append(lime_explanation_text.strip())
        lines.append("")

    # LIME feature weights
    if lime_explanation and "top_features" in lime_explanation:
        lines.append("LIME Feature Weights")
        lines.append("-" * 30)
        for f in lime_explanation["top_features"]:
            lines.append(f"{f['feature']}: {f['importance']}")
        lines.append("")

    lines.append("Recommendations: Based on this analysis, consider the following:")
    lines.append("- Improve fairness metrics if any are flagged as biased.")
    lines.append("- Use differential privacy techniques if not already applied.")
    lines.append("- Avoid overlap between train/test datasets to reduce data leakage.")
    lines.append("")


    return "\n".join(lines)